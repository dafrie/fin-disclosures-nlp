{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/Users/david/Nextcloud/Dokumente/Education/Uni Bern/Master Thesis/Analyzing Financial Climate Disclosures with NLP/v2/labelling/\"\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, \"labels_training_21052021_full_II.csv\"))\n",
    "\n",
    "CRO_LEVEL = \"cro_sub_type\" # cro, cro_sub_type\n",
    "CATEGORY_CODES = [\"ACUTE\", \"CHRON\", \"POLICY\", \"MARKET\", \"REPUT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged ID 4 with 2 rows...\n",
      "Merged ID 7 with 2 rows...\n",
      "Merged ID 15 with 2 rows...\n",
      "Merged ID 17 with 2 rows...\n",
      "Merged ID 21 with 2 rows...\n",
      "Merged ID 31 with 2 rows...\n",
      "Merged ID 100 with 2 rows...\n",
      "Merged ID 101 with 2 rows...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "df[\"id\"] =  df.apply(lambda row: f\"{row.report_id}_{str(row.page)}_{str(row.paragraph_no)}\", axis=1)\n",
    "df = df.drop(\"Unnamed: 0\", axis=1, errors='ignore')\n",
    "\n",
    "def parse_comment(row):\n",
    "    if not row.comment or pd.isna(row.comment):\n",
    "        return np.NaN\n",
    "    span_id = re.search('<id>(.*)</id>', row.comment)\n",
    "    if span_id:\n",
    "        span_id = span_id.group(1)\n",
    "    else:\n",
    "        span_id = np.NaN\n",
    "    return span_id\n",
    "\n",
    "df[\"span_id\"] = df.apply(lambda row: parse_comment(row), axis=1)\n",
    "\n",
    "for span_id in df.span_id.unique():\n",
    "    if not pd.isna(span_id):\n",
    "        rows = df[df.span_id == span_id]\n",
    "        len_rows = len(rows)\n",
    "        text = \"\\n\".join(rows.text.tolist())\n",
    "        df = df.drop(rows.index[1:])\n",
    "        df.loc[df.span_id == span_id, \"text\"] = text\n",
    "        print(f\"Merged ID {span_id} with {len_rows} rows...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df.groupby([\"id\"]).first().text\n",
    "\n",
    "df.loc[df.cro_sub_type != df.cro_sub_type, \"cro_sub_type\"] = \"missing\"\n",
    "labels = pd.crosstab(df.id, df.cro_sub_type, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels[CATEGORY_CODES]\n",
    "labels['text'] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cro_sub_type\n",
       "ACUTE                                                   140\n",
       "CHRON                                                    58\n",
       "POLICY                                                   47\n",
       "MARKET                                                   43\n",
       "REPUT                                                    26\n",
       "text      Risks related to climate change refer to the p...\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cro_sub_type\n",
       "ACUTE                                                   141\n",
       "CHRON                                                    57\n",
       "POLICY                                                   47\n",
       "MARKET                                                   40\n",
       "REPUT                                                    26\n",
       "text      Risks related to climate change refer to the p...\n",
       "dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.to_csv(os.path.join(DATA_DIR, f\"train_explicit_labels_V.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALID / TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File containing \"cleaned\" positives (and resulting good negatives!)\n",
    "df1 = pd.read_excel(os.path.join(DATA_DIR, \"old_train_positives_BS_v02.xls\"))\n",
    "df1 = df1.set_index(\"id\")\n",
    "df1 = df1.rename(columns={\"REPUTATION\": \"REPUT\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From here, we only want to get the additional negatives\n",
    "df2 = pd.read_pickle(os.path.join(DATA_DIR, \"Firm_AnnualReport_Labels_Training.pkl\"))\n",
    "df2[\"id\"] = df2.apply(lambda row: f\"{row.report_id}_{str(row.page)}_{str(row.paragraph_no)}\", axis=1)\n",
    "\n",
    "df2_docs = df2.groupby([\"id\"]).first().text\n",
    "df2[\"cro_sub_type_combined\"].loc[df2[\"cro_sub_type_combined\"] != df2[\"cro_sub_type_combined\"]] = \"missing\"\n",
    "df2_labels = pd.crosstab(df2.id, df2[\"cro_sub_type_combined\"], dropna=False)\n",
    "df2_labels = df2_labels.rename(columns={\"REPUTATION\": \"REPUT\"})\n",
    "df2_labels = df2_labels[CATEGORY_CODES]\n",
    "df2 = df2_labels.join(df2_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_idx = df2.index.difference(df1.index)\n",
    "df_combined = df1.append(df2.loc[missing_idx, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File containing \"cleaned\" positives (and resulting good negatives!)\n",
    "df3 = pd.read_excel(os.path.join(DATA_DIR, \"old_test_positives_DF.xls\"))\n",
    "df3 = df3.rename(columns={\"REPUTATION\": \"REPUT\", \"Unnamed: 0\": \"id\"})\n",
    "df3 = df3.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From here, we only want to get the additional negatives\n",
    "df4 = pd.read_pickle(os.path.join(DATA_DIR, \"Firm_AnnualReport_Labels_Test.pkl\"))\n",
    "df4[\"id\"] = df4.apply(lambda row: f\"{row.report_id}_{str(row.page)}_{str(row.paragraph_no)}\", axis=1)\n",
    "\n",
    "df4_docs = df4.groupby([\"id\"]).first().text\n",
    "df4[\"cro_sub_type_combined\"].loc[df4[\"cro_sub_type_combined\"] != df4[\"cro_sub_type_combined\"]] = \"missing\"\n",
    "df4_labels = pd.crosstab(df4.id, df4[\"cro_sub_type_combined\"], dropna=False)\n",
    "df4_labels = df4_labels.rename(columns={\"REPUTATION\": \"REPUT\"})\n",
    "df4_labels = df4_labels[CATEGORY_CODES]\n",
    "df4 = df4_labels.join(df4_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_idx2 = df4.index.difference(df3.index)\n",
    "df_combined2 = df3.append(df4.loc[missing_idx2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_final = pd.concat([df_combined, df_combined2])\n",
    "df_combined_final.to_csv(os.path.join(DATA_DIR, f\"test_realistic.csv\"))\n",
    "\n",
    "df_combined_final_optimistic = pd.concat([df1, df3])\n",
    "df_combined_final_optimistic.to_csv(os.path.join(DATA_DIR, \"test_optimistic.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_final_optimistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_final.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_final[\"count\"] = df_combined_final.apply(lambda row: len(tokenizer.tokenize(str(row.text))), axis=1)\n",
    "print(df_combined_final[\"count\"].describe())\n",
    "df_combined_final[\"count\"].plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_final_optimistic[\"count\"] = df_combined_final_optimistic.apply(lambda row: len(tokenizer.tokenize(str(row.text))), axis=1)\n",
    "print(df_combined_final_optimistic[\"count\"].describe())\n",
    "df_combined_final_optimistic[\"count\"].plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world! This is an   \\n    awesome test of the BERT tokenizer.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Tokens: \", tokens)\n",
    "print(\"ID's: \", ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(os.path.join(DATA_DIR, \"train_explicit_labels.csv\"))\n",
    "df_2 = pd.read_csv(os.path.join(DATA_DIR, \"train_explicit_labels_cleaned.csv\"))\n",
    "df_3 = pd.read_csv(os.path.join(DATA_DIR, \"train_explicit_labels_II.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
