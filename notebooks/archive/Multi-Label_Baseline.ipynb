{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Label Baseline Models\n",
    "\n",
    "This is the notebook containing End-To-End models for multi-label classification of CRO's, for both level using TF-IDF as input features for a set of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## CONFIG ##############################\n",
    "# Task config\n",
    "TASK = \"binary\" #@param [\"multi-label\", \"binary\"]\n",
    "CATEGORY_LEVEL = 'cro_sub_type_combined' #@param [\"cro\", \"cro_sub_type_combined\"]\n",
    "MODEL_TYPE = \"baseline\" #@param [\"baseline\", \"transformer\"]\n",
    "MODEL_NAME = \"svm\"\n",
    "\n",
    "# Dataset config\n",
    "FILTER_OP = True #@param { type: \"boolean\"}\n",
    "SCENARIO = \"efficient-realistic\" #@param [ \"optimistic\", \"efficient-realistic\", \"realistic\"]\n",
    "\n",
    "# Evaluation metric config. See for context: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "AVERAGING_STRATEGY = 'macro' #@param [\"micro\",  \"macro\", \"weighted\"]\n",
    "\n",
    "RESULTS_FILE_NAME = f\"{CATEGORY_LEVEL}_{TASK}_results.csv\"\n",
    "\n",
    "# To make the notebook reproducible (not guaranteed for pytorch on different releases/platforms!)\n",
    "SEED_VALUE = 42\n",
    "\n",
    "##################\n",
    "\n",
    "SCORING_METRIC = 'average_precision'  # 'average_precision', 'roc_auc'. AP is equal to Precision/Recall AUC! See for discussion: https://github.com/scikit-learn/scikit-learn/issues/5992\n",
    "\n",
    "####################################################################\n",
    "LOCAL_DIR = \"/Users/david/Nextcloud/Dokumente/Education/Uni Bern/Master Thesis/Analyzing Financial Climate Disclosures with NLP/Methodology/\"\n",
    "COLAB_DIR = \"/content/drive/MyDrive/fin-disclosures-nlp\"\n",
    "\n",
    "if SCENARIO == \"optimistic\":\n",
    "  TRAIN_NEG_SAMPLING_STRATEGY = \"None\"\n",
    "  TEST_NEG_SAMPLING_STRATEGY = \"None\"\n",
    "\n",
    "elif SCENARIO == \"efficient-realistic\":\n",
    "  TRAIN_NEG_SAMPLING_STRATEGY = \"only_OP\"\n",
    "  TEST_NEG_SAMPLING_STRATEGY = \"all\"\n",
    "\n",
    "elif SCENARIO == \"realistic\":\n",
    "  TRAIN_NEG_SAMPLING_STRATEGY = \"all\"\n",
    "  TEST_NEG_SAMPLING_STRATEGY = \"all\"\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"task\": TASK,\n",
    "    \"category_level\": CATEGORY_LEVEL,\n",
    "    \"model_type\": MODEL_TYPE,\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"scenario\": SCENARIO,\n",
    "    \"seed_value\": SEED_VALUE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_running_in_colab:\n",
    "  # Load Google drive where the data and models are stored\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "if is_running_in_colab:\n",
    "  # Install transformers library + datasets helper\n",
    "  !pip install transformers --quiet\n",
    "  !pip install datasets --quiet\n",
    "  !pip install optuna --quiet\n",
    "\n",
    "  # Latex for output\n",
    "  ! apt install texlive-latex-recommended -qq\n",
    "  ! apt install texlive-latex-extra -qq\n",
    "  ! apt install dvipng -qq\n",
    "  ! apt install cm-super -qq\n",
    "\n",
    "  # Load repository\n",
    "\n",
    "  !git clone https://github.com/dafrie/fin-disclosures-nlp.git    \n",
    "  %cd /content/fin-disclosures-nlp\n",
    "  !git pull\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append('..')\n",
    "\n",
    "from data import constants\n",
    "from data import cro_dataset\n",
    "from data import dataframe_preparation\n",
    "from data import evaluation\n",
    "\n",
    "DIR = COLAB_DIR if is_running_in_colab else LOCAL_DIR\n",
    "DATA_DIR = os.path.join(DIR, \"data\", \"labels\")\n",
    "MODELS_DIR = os.path.join(DIR, \"models\", MODEL_TYPE)\n",
    "RESULTS_FILE_PATH = os.path.join(DIR, 'results', RESULTS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load/Initialize results file\n",
    "results = evaluation.Results(RESULTS_FILE_PATH, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset. Train: 1103, Test: 28209, Dim: 1\n"
     ]
    }
   ],
   "source": [
    "train_docs, train_doc_labels, test_docs, test_doc_labels = cro_dataset.prepare_datasets(\n",
    "    data_dir=DATA_DIR,\n",
    "    task=TASK, \n",
    "    cro_category_level=CATEGORY_LEVEL, \n",
    "    should_filter_op=FILTER_OP, \n",
    "    train_neg_sampling_strategy=TRAIN_NEG_SAMPLING_STRATEGY, \n",
    "    test_neg_sampling_strategy=TEST_NEG_SAMPLING_STRATEGY, \n",
    "    seed_value=SEED_VALUE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('bow',\n",
       "                                        CountVectorizer(strip_accents='ascii')),\n",
       "                                       ('tfidf', TfidfTransformer()),\n",
       "                                       ('classifier',\n",
       "                                        SVC(class_weight='balanced',\n",
       "                                            probability=True,\n",
       "                                            random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'bow__max_features': [200],\n",
       "                          'bow__ngram_range': [(1, 2)],\n",
       "                          'bow__stop_words': ['english'],\n",
       "                          'bow__tokenizer': [<function spacy_tokenizer at 0x7fc06859e550>],\n",
       "                          'classifier__C': [10],\n",
       "                          'classifier__kernel': ['linear'],\n",
       "                          'tfidf__use_idf': [True]}],\n",
       "             scoring=make_scorer(average_precision_score, average=macro))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, average_precision_score, roc_auc_score\n",
    "\n",
    "# Custom scorer so we can pass in the averaging strategy\n",
    "avg_scorer = make_scorer(average_precision_score if SCORING_METRIC == 'average_precision' else roc_auc_score, average=AVERAGING_STRATEGY)\n",
    "\n",
    "svc_clf = SVC(probability=True, random_state=SEED_VALUE, class_weight=\"balanced\") # Balanced: n_samples / (n_classes * np.bincount(y)). Since we are doing OneVsRest, this should be giving correct weights!\n",
    "# Wrap with OvR in case of multi-label\n",
    "multi_label_clf = OneVsRestClassifier(svc_clf)\n",
    "\n",
    "pipeline_svm = Pipeline([\n",
    "    ('bow', CountVectorizer(strip_accents = 'ascii')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', multi_label_clf if TASK == \"multi-label\" else svc_clf),\n",
    "     ])\n",
    "\n",
    "# Parameters to tune automatically with a grid search\n",
    "# Note: The nested estimator is accessible via the __estimator identifier\n",
    "param_svm = [\n",
    "  {\n",
    "      'bow__tokenizer': [dataframe_preparation.spacy_tokenizer, None],\n",
    "      'bow__stop_words': ['english', None],\n",
    "      'bow__ngram_range': [(1, 1), (1, 2)],\n",
    "      'bow__max_features': [50, 200],\n",
    "      'tfidf__use_idf': (True), \n",
    "      'classifier__estimator__C' if TASK == \"multi-label\" else 'classifier__C': [1, 10, 100], \n",
    "      'classifier__estimator__kernel' if TASK == \"multi-label\" else 'classifier__kernel': ['linear', 'rbf']},\n",
    "]\n",
    "\n",
    "# TODO: Remove\n",
    "param_svm = [\n",
    "  {\n",
    "      'bow__tokenizer': [dataframe_preparation.spacy_tokenizer],\n",
    "      'bow__stop_words': ['english'],\n",
    "      'bow__ngram_range': [(1, 2)],\n",
    "      'bow__max_features': [200],\n",
    "      'tfidf__use_idf': [True], \n",
    "      'classifier__estimator__C' if TASK == \"multi-label\" else 'classifier__C': [10], \n",
    "      'classifier__estimator__kernel' if TASK == \"multi-label\" else 'classifier__kernel': ['linear']},\n",
    "]\n",
    "\n",
    "grid_clf = GridSearchCV(\n",
    "    pipeline_svm,\n",
    "    param_grid=param_svm,\n",
    "    refit=True,\n",
    "    n_jobs=-1, \n",
    "    scoring=avg_scorer,\n",
    ")\n",
    "\n",
    "# Grid search fitting\n",
    "grid_clf.fit(train_docs, train_doc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid_clf.cv_results_)\n",
    "\n",
    "print(f\"Best {SCORING_METRIC} score: {grid_clf.best_score_}\")\n",
    "print(f\"Best params: \\n{grid_clf.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â train_preds = grid_clf.predict(train_docs)\n",
    "train_preds_prob = grid_clf.predict_proba(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_scores, best_roc_threshold, best_pr_threshold = evaluation.threshold_moving_report(train_doc_labels, train_preds_prob)\n",
    "results.log_experiment(train_eval_scores, prefix=\"train\")\n",
    "results.log_experiment({ \"best_pr_threshold\": best_pr_threshold.values(), \"best_roc_threshold\": best_roc_threshold.values()}, prefix=\"train\")\n",
    "train_eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for test\n",
    "test_preds_prob = grid_clf.predict_proba(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval_scores = evaluation.test_evaluation_report(test_doc_labels, test_preds_prob, best_pr_threshold.values(), averaging=AVERAGING_STRATEGY)\n",
    "results.log_experiment(test_eval_scores, prefix=\"test\")\n",
    "test_eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "SAVE_MODEL = False\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    with open(os.path.join(MODELS_DIR), f\" {TASK}_svm_{CATEGORY_LEVEL}.pkl\", 'wb') as f:\n",
    "        grid_clf.label_list = label_list\n",
    "        pickle.dump(grid_clf, f, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
