{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_a.drop(df_a.query(\"cro == 'OP'\"))# Calculation of ICR scores\n",
    "\n",
    "Ressources: https://doi.org/10.1177%2F1609406919899220\n",
    "\n",
    "\"Researchers often cite Landis and Koch’s (1977) recommendation of interpreting values less than 0 as indicating no, between 0 and 0.20 as slight, 0.21 and 0.40 as fair, 0.41 and 0.60 as moderate, 0.61 and 0.80 as substantial, and 0.81 and 1 as nearly perfect agreement.\n",
    "\n",
    "All such guidelines are ultimately arbitrary, and the researcher must judge what represents acceptable agreement for a particular study. Studies that influence important medical, policy, or financial decisions arguably merit a higher ICR threshold than exploratory academic research (Hruschka et al., 2004; Lombard et al., 2002). For instance, McHugh (2012) proposes a more conservative system of acceptability thresholds when using Cohen’s kappa coefficients in the context of clinical decision-making. Whatever interpretative framework is chosen should be stipulated in advance and not decided post hoc after results are viewed.\n",
    "\"\n",
    "\n",
    "History:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## CONFIG ########################\n",
    "CATEGORY_LEVEL = \"cro_sub_type_combined\" # cro, cro_sub_type, cro_sub_type_combined\n",
    "JOIN_STRATEGY = \"inner\" # inner, outer, left, right\n",
    "##################################################\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from nltk.metrics import masi_distance, binary_distance\n",
    "\n",
    "sys.path.append('../..')\n",
    "import data\n",
    "from data.labels_postprocessing import process\n",
    "\n",
    "# Load files\n",
    "df_a = pd.read_pickle(\"/Users/david/Nextcloud/Dokumente/Education/Uni Bern/Master Thesis/Analyzing Financial Climate Disclosures with NLP/Labelling/annual reports/icr/initial 15/Firm_AnnualReport_Labels_DF.pkl\")\n",
    "df_b = pd.read_pickle(\"/Users/david/Nextcloud/Dokumente/Education/Uni Bern/Master Thesis/Analyzing Financial Climate Disclosures with NLP/Labelling/annual reports/icr/initial 15/Firm_AnnualReport_Labels_TS.pkl\")\n",
    "#df_b.to_csv(\"/Users/david/Nextcloud/Dokumente/Education/Uni Bern/Master Thesis/Analyzing Financial Climate Disclosures with NLP/Labelling/annual reports/icr/initial 15/Firm_AnnualReport_Labels_TS.csv\")\n",
    "\n",
    "# Run postprocessing\n",
    "df_a = process(df_a)\n",
    "df_b = process(df_b)\n",
    "\n",
    "# Set id\n",
    "id_columns = ['report_id', 'page', 'paragraph_no']\n",
    "df_a[\"id\"] = df_a.apply(lambda row: \"_\".join([str(row[c]) for c in id_columns]), axis=1)\n",
    "df_b[\"id\"] = df_b.apply(lambda row: \"_\".join([str(row[c]) for c in id_columns]), axis=1)\n",
    "\n",
    "# Special case: Remove \"indirect\" since those were not labelled by B\n",
    "df_a = df_a.query(\"indirect == False\")\n",
    "df_a = df_a[df_a.comment.str.contains(\"inversed\").replace(np.nan,False) == False] # Also, the \"inversed\" disclosures\n",
    "\n",
    "# Remove erroneously labelled \"interview\" with a customer\n",
    "df_b = df_b.iloc[3:]\n",
    "\n",
    "# Only keep labelled paragraphs\n",
    "df_a = df_a.query(\"cro == ['PR', 'TR', 'OP']\")\n",
    "df_b = df_b.query(\"cro == ['PR', 'TR', 'OP']\")\n",
    "\n",
    "# Remove OP and negative examples (that only A labelled)\n",
    "df_a = df_a.query(\"cro == ['PR', 'TR']\")\n",
    "df_b = df_b.query(\"cro == ['PR', 'TR']\")\n",
    "\n",
    "paragraphs_a = pd.crosstab(df_a.id, df_a[CATEGORY_LEVEL], dropna=False)\n",
    "paragraphs_b = pd.crosstab(df_b.id, df_b[CATEGORY_LEVEL], dropna=False)\n",
    "\n",
    "paragraphs = paragraphs_a.join(paragraphs_b, how=JOIN_STRATEGY, lsuffix='_a', rsuffix='_b')\n",
    "paragraphs = paragraphs.replace(np.nan, 0)\n",
    "paragraphs = (paragraphs > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's Alpha: \t0.69\n",
      "Cohen's Kappa: \t\t0.667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "columns = [c for c in paragraphs.columns if c.endswith(\"_a\")]\n",
    "mlb.fit([columns])\n",
    "\n",
    "labels = []\n",
    "for index, row in paragraphs.iterrows():\n",
    "    labels_a = row[[c for c in paragraphs.columns if c.endswith(\"_a\")]].to_numpy()\n",
    "    labels_b = row[[c for c in paragraphs.columns if c.endswith(\"_b\")]].to_numpy()\n",
    "    labels_a = np.array([labels_a])\n",
    "    labels_b = np.array([labels_b])\n",
    "    labels_a = mlb.inverse_transform(labels_a)\n",
    "    labels_b = mlb.inverse_transform(labels_b)\n",
    "    \n",
    "    a = ('coder_a', index, frozenset(labels_a))\n",
    "    b = ('coder_b', index, frozenset(labels_b))\n",
    "\n",
    "    labels.append(a)\n",
    "    labels.append(b)\n",
    "\n",
    "task = AnnotationTask(data=labels, distance = binary_distance)\n",
    "print(f\"Krippendorff's Alpha: \\t{round(task.alpha(),3)}\")\n",
    "print(f\"Cohen's Kappa: \\t\\t{round(task.kappa(),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
