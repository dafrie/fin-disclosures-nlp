{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of ICR scores\n",
    "\n",
    "Ressources: https://doi.org/10.1177%2F1609406919899220\n",
    "\n",
    "\"Researchers often cite Landis and Koch’s (1977) recommendation of interpreting values less than 0 as indicating no, between 0 and 0.20 as slight, 0.21 and 0.40 as fair, 0.41 and 0.60 as moderate, 0.61 and 0.80 as substantial, and 0.81 and 1 as nearly perfect agreement.\n",
    "\n",
    "All such guidelines are ultimately arbitrary, and the researcher must judge what represents acceptable agreement for a particular study. Studies that influence important medical, policy, or financial decisions arguably merit a higher ICR threshold than exploratory academic research (Hruschka et al., 2004; Lombard et al., 2002). For instance, McHugh (2012) proposes a more conservative system of acceptability thresholds when using Cohen’s kappa coefficients in the context of clinical decision-making. Whatever interpretative framework is chosen should be stipulated in advance and not decided post hoc after results are viewed.\n",
    "\"\n",
    "\n",
    "History:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's Alpha: \t0.56\n",
      "Cohen's Kappa: \t\t0.571\n"
     ]
    }
   ],
   "source": [
    "################## CONFIG ########################\n",
    "CATEGORY_LEVEL = \"cro_sub_type_combined\" # cro, cro_sub_type, cro_sub_type_combined\n",
    "JOIN_STRATEGY = \"inner\" # inner, outer, left, right\n",
    "##################################################\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from nltk.metrics import masi_distance\n",
    "\n",
    "sys.path.append('..')\n",
    "import data\n",
    "from data.labels_postprocessing import process\n",
    "\n",
    "# Load files\n",
    "df_a = pd.read_pickle(\"/Users/david/Nextcloud/Dokumente/Education/Uni Bern/Master Thesis/Analyzing Financial Climate Disclosures with NLP/Labelling/annual reports/icr/initial 15/Firm_AnnualReport_Labels_DF.pkl\")\n",
    "df_b = pd.read_pickle(\"/Users/david/Nextcloud/Dokumente/Education/Uni Bern/Master Thesis/Analyzing Financial Climate Disclosures with NLP/Labelling/annual reports/icr/initial 15/Firm_AnnualReport_Labels_TS.pkl\")\n",
    "df_b.to_csv(\"/Users/david/Nextcloud/Dokumente/Education/Uni Bern/Master Thesis/Analyzing Financial Climate Disclosures with NLP/Labelling/annual reports/icr/initial 15/Firm_AnnualReport_Labels_TS.csv\")\n",
    "\n",
    "\n",
    "# Run postprocessing\n",
    "df_a = process(df_a)\n",
    "df_b = process(df_b)\n",
    "\n",
    "# Set id\n",
    "id_columns = ['report_id', 'page', 'paragraph_no']\n",
    "df_a[\"id\"] = df_a.apply(lambda row: \"_\".join([str(row[c]) for c in id_columns]), axis=1)\n",
    "df_b[\"id\"] = df_b.apply(lambda row: \"_\".join([str(row[c]) for c in id_columns]), axis=1)\n",
    "\n",
    "# Special case: Remove \"indirect\" since those were not labelled by B\n",
    "df_a = df_a.query(\"indirect == False\")\n",
    "\n",
    "paragraphs_a = pd.DataFrame(df_a.groupby(id_columns).size(), columns=['count'])\n",
    "paragraphs_b = pd.DataFrame(df_b.groupby(id_columns).size(), columns=['count'])\n",
    "paragraphs = paragraphs_a.join(paragraphs_b, on=id_columns, how=JOIN_STRATEGY, lsuffix='_a', rsuffix='_b')\n",
    "total_paragraphs = paragraphs_a.join(paragraphs_b, on=id_columns, how=\"outer\", lsuffix='_a', rsuffix='_b')\n",
    "\n",
    "def invert_labels(df, paragraph_id):\n",
    "    labels = df[df.id == paragraph_id][CATEGORY_LEVEL].unique()\n",
    "    result = []\n",
    "    # TODO: Sort?\n",
    "    for l in labels:\n",
    "        result.append(l)\n",
    "    return frozenset(result if len(result) else ['NaN'])\n",
    "\n",
    "labels = []\n",
    "for index, row in paragraphs.iterrows():\n",
    "    paragraph_id = \"_\".join(str(v) for v in index)\n",
    "    a = ('coder_a', paragraph_id, invert_labels(df_a, paragraph_id))\n",
    "    b = ('coder_b', paragraph_id, invert_labels(df_b, paragraph_id))\n",
    "    labels.append(a)\n",
    "    labels.append(b)\n",
    "    \n",
    "    \n",
    "task = AnnotationTask(data=labels, distance = masi_distance)\n",
    "print(f\"Krippendorff's Alpha: \\t{round(task.alpha(),3)}\")\n",
    "print(f\"Cohen's Kappa: \\t\\t{round(task.kappa(),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 63 100 35\n"
     ]
    }
   ],
   "source": [
    "print(len(paragraphs_a), len(paragraphs_b), len(total_paragraphs), len(paragraphs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
