{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "from data import constants\n",
    "\n",
    "############################## CONFIG ##############################\n",
    "# To make the notebook reproducible (not guaranteed for pytorch on different releases/platforms!)\n",
    "SEED_VALUE = 2\n",
    "FILTER_OP = False\n",
    "FILTER_WEAK = False\n",
    "\n",
    "####################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weights of {0: 1.0, 1: 25.32422586520947}\n"
     ]
    }
   ],
   "source": [
    "# Read training and test data\n",
    "df_train = pd.read_pickle(\"/Users/david/Nextcloud/Dokumente/Education/Uni Bern/Master Thesis/Analyzing Financial Climate Disclosures with NLP/Labelling/annual reports/Firm_AnnualReport_Labels_Training.pkl\")\n",
    "df_test = pd.read_pickle(\"/Users/david/Nextcloud/Dokumente/Education/Uni Bern/Master Thesis/Analyzing Financial Climate Disclosures with NLP/Labelling/annual reports/Firm_AnnualReport_Labels_Test.pkl\")\n",
    "\n",
    "# df_train = df_train.sample(500)\n",
    "# df_test = df_test.sample(500)\n",
    "\n",
    "# Set id\n",
    "id_columns = ['report_id', 'page', 'paragraph_no']\n",
    "df_train[\"id\"] = df_train.apply(lambda row: \"_\".join([str(row[c]) for c in id_columns]), axis=1)\n",
    "df_test[\"id\"] = df_test.apply(lambda row: \"_\".join([str(row[c]) for c in id_columns]), axis=1)\n",
    "\n",
    "if FILTER_OP:\n",
    "  df_train.cro.loc[df_train.cro == \"OP\"] = np.nan\n",
    "  df_test.cro.loc[df_test.cro == \"OP\"] = np.nan\n",
    "\n",
    "if FILTER_WEAK:\n",
    "  df_train = df_train.query(\"neg_type != 'weak'\")\n",
    "  df_test = df_test.query(\"neg_type != 'weak'\")\n",
    "\n",
    "train_docs = df_train.groupby([\"id\"]).first().text\n",
    "train_doc_labels = df_train.groupby([\"id\"]).cro.count()\n",
    "train_doc_labels = (train_doc_labels > 0) * 1\n",
    "assert len(train_docs) == len(train_doc_labels)\n",
    "\n",
    "test_docs = df_test.groupby([\"id\"]).first().text\n",
    "test_doc_labels = df_test.groupby([\"id\"]).cro.count()\n",
    "test_doc_labels = (test_doc_labels > 0) * 1\n",
    "assert len(test_docs) == len(test_doc_labels)\n",
    "\n",
    "# Calculate weights\n",
    "weights = {0:1.0, 1: len(train_doc_labels) / train_doc_labels.sum()}\n",
    "print(f\"Using weights of {weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Preprocessing (Lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "docs_train, labels_train = shuffle(train_docs, train_doc_labels, random_state=SEED_VALUE)\n",
    "\n",
    "weights = {0:1.0, 1: len(train_doc_labels) / train_doc_labels.sum()}\n",
    "\n",
    "pipeline_svm = Pipeline([\n",
    "    ('bow', CountVectorizer(strip_accents = 'ascii'), tokenizer=LemmaTokenizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', SVC(probability=True, random_state=SEED_VALUE, class_weight=weights)),\n",
    "    ])\n",
    "\n",
    "# Parameters to tune automatically with a grid search\n",
    "# Note: The nested estimator is accessible via the __estimator identifier\n",
    "param_svm = [\n",
    "  {\n",
    "      'bow__ngram_range': [(1, 2)],\n",
    "      'bow__max_features': [None, 100, 200],\n",
    "      'bow__stop_words': ['english', None],\n",
    "       'tfidf__use_idf': [True],\n",
    "      'classifier__C': [1, 10, 100],\n",
    "      'classifier__kernel': ['linear', 'rbf'],\n",
    "  },\n",
    "]\n",
    "\n",
    "grid_clf = GridSearchCV(\n",
    "    pipeline_svm,\n",
    "    param_grid=param_svm,\n",
    "    refit=True,\n",
    "    n_jobs=-1, \n",
    "    scoring='roc_auc',\n",
    "    # cv=StratifiedKFold(label_train, n_folds=5),\n",
    ")\n",
    "\n",
    "# Grid search fitting\n",
    "grid_clf.fit(docs_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid_clf.cv_results_)\n",
    "\n",
    "print(f\"Best score: {grid_clf.best_score_}\")\n",
    "print(f\"Best params: \\n{grid_clf.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for test\n",
    "preds = grid_clf.predict(test_docs)\n",
    "preds_prob = grid_clf.predict_proba(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report, roc_curve, roc_auc_score, accuracy_score, matthews_corrcoef\n",
    "\n",
    "preds = preds_prob[:,1]\n",
    "labels = test_doc_labels\n",
    "\n",
    "test_roc_auc = roc_auc_score(labels, preds)\n",
    "print(\"Test ROC AuC: \", test_roc_auc)\n",
    "\n",
    "threshold = 0.8\n",
    "preds_bool = (preds > threshold)\n",
    "\n",
    "label_list = [\"irrelevant\", \"relevant\"]\n",
    "print(classification_report(labels, preds_bool, target_names=label_list))\n",
    "\n",
    "acc = accuracy_score(labels, preds_bool)\n",
    "matthews_corr = matthews_corrcoef(labels, preds_bool)\n",
    "\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Matthews: {matthews_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_cm(labels, predictions, p=0.8):\n",
    "  cm = confusion_matrix(labels, predictions > p)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.0%}'.format(p))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "plot_cm(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpr, tpr, thresholds = roc_curve(testy, probs)\n",
    "# pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
    "# pyplot.plot(fpr, tpr, marker='.')\n",
    "# pyplot.show()\n",
    "# auc_score = roc_auc_score(testy, probs)\n",
    "# print('AUC: %.3f' % auc_score)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "  fp, tp, _ = roc_curve(labels, predictions)\n",
    "\n",
    "  fig = plt.plot(100 * fp, 100 * tp, label=name, linewidth=2, **kwargs)\n",
    "  plt.xlabel('False positives [%]')\n",
    "  plt.ylabel('True positives [%]')\n",
    "  ax = plt.gca()\n",
    "  ax.set_aspect('equal')\n",
    "  plt.legend(loc='lower right')\n",
    "  return fig\n",
    "\n",
    "# plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color=colors[0])\n",
    "fig = plot_roc(\"Test\", labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_score = grid_clf.decision_function(test_docs)\n",
    "average_precision = average_precision_score(labels, y_score)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "\n",
    "disp = plot_precision_recall_curve(grid_clf, test_docs, test_doc_labels)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "test_precision, test_recall, test_thresholds = precision_recall_curve(labels, preds)\n",
    "test_auc_score = auc(test_recall, test_precision)\n",
    "print(\"Test Precision/Recall AuC: \", test_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
