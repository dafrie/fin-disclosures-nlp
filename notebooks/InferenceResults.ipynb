{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('..')\n",
    "from data import constants\n",
    "from data import dataframe_preparation\n",
    "from data.utils import tables\n",
    "\n",
    "# Setup seaborn\n",
    "sns.set_theme(style=\"ticks\", rc={'text.usetex' : True})\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Read main file\n",
    "df = pd.read_pickle(\"/Users/david/Nextcloud/Dokumente/Education/Uni Bern/Master Thesis/Analyzing Financial Climate Disclosures with NLP/Data/stoxx_inference/Firm_AnnualReport_Paragraphs.pkl\")\n",
    "id_columns = ['report_id', 'page_no', 'paragraph_no']\n",
    "df[\"id\"] = df.apply(lambda row: \"_\".join([str(row[c]) for c in id_columns]), axis=1)\n",
    "df = df.set_index([\"id\"])\n",
    "assert df.index.is_unique, \"Index is not unique. Check the data!\"\n",
    "\n",
    "first_stage = df['1stage_preds_labels'].apply(lambda x: np.array(x[1]))\n",
    "second_stage = df['2stage_preds_labels'].apply(lambda x: np.array(x))\n",
    "df[\"labels\"] = first_stage * second_stage\n",
    "df[constants.cro_sub_category_labels] = pd.DataFrame(df.labels.tolist(), index= df.index)\n",
    "df[constants.cro_category_labels[0]] = df.labels.apply(lambda x: any(x[0:2]))\n",
    "df[constants.cro_category_labels[1]] = df.labels.apply(lambda x: any(x[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read master for scaling\n",
    "df_master = pd.read_csv(\"/Users/david/Nextcloud/Dokumente/Education/Uni Bern/Master Thesis/Analyzing Financial Climate Disclosures with NLP/Data/stoxx_inference/Firm_AnnualReport.csv\")\n",
    "df_master = df_master.set_index(\"id\")\n",
    "df_master['icb_industry'] = df_master['icb_industry'].str.slice(3)\n",
    "df_master['country'] = df_master['country'].str.upper()\n",
    "df_master = df_master.rename(columns={\"year\": \"Year\"})\n",
    "df_reports_count = df_master.groupby('Year')['is_inferred'].sum()\n",
    "df = pd.merge(df, df_master, how=\"left\", left_on=\"report_id\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of relevant pages per report:  45.46969696969697\n"
     ]
    }
   ],
   "source": [
    "print(\"Average of relevant pages per report: \", len(df.groupby([\"report_id\", \"page_no\"]).count())/len(df_master))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_level = \"cro\" # [\"cro\", \"cro_sub_type\"]\n",
    "categories = constants.cro_categories if category_level == \"cro\" else constants.cro_sub_categories\n",
    "cro_category_labels = [c[\"label\"] for c in categories]\n",
    "\n",
    "export_dir = os.path.join(\"/Users/david/Nextcloud/Dokumente/Education/Uni Bern/Master Thesis/Analyzing Financial Climate Disclosures with NLP/Thesis/figures/\")\n",
    "\n",
    "colors = [c[\"color\"] for c in categories]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reports: 792, processed reports: 772. Missing reports:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'de_bayer-AR_2001',\n",
       " 'de_deutsche_telekom-AR_1999',\n",
       " 'de_deutsche_telekom-AR_2000',\n",
       " 'de_deutsche_telekom-AR_2001',\n",
       " 'de_sap-AR_1999',\n",
       " 'de_sap-AR_2000',\n",
       " 'de_sap-AR_2001',\n",
       " 'de_sap-AR_2002',\n",
       " 'dk_novo_nordisk_b-AR_2001',\n",
       " 'dk_novo_nordisk_b-AR_2002',\n",
       " 'fr_airbus-AR_2007',\n",
       " 'fr_airbus-AR_2008',\n",
       " 'fr_airbus-AR_2009',\n",
       " 'gb_bp-AR_2017',\n",
       " 'gb_lloyds_banking_grp-AR_2000',\n",
       " 'gb_lloyds_banking_grp-AR_2001',\n",
       " 'gb_prudential-AR_1999',\n",
       " 'gb_reckitt_benckiser_grp-AR_2005',\n",
       " 'gb_vodafone_grp-AR_2012',\n",
       " 'nl_asml_hldg-AR_2001'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_inferred_reports = set(df_master.index) - set(df.report_id)\n",
    "print(f\"Total number of reports: {len(df_master.index)}, processed reports: {len(df.report_id.unique())}. Missing reports:\")\n",
    "missing_inferred_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique pages: 36012, paragraphs: 628001\n"
     ]
    }
   ],
   "source": [
    "print(f'Unique pages: {len(df.groupby([\"report_id\", \"page_no\"]).count())}, paragraphs: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " df[\"is_climaterisk\"] = df.apply(lambda x: x[\"1stage_preds_labels\"][1] > 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First stage positives: 3713\n"
     ]
    }
   ],
   "source": [
    " print(f\"First stage positive paragraphs: {df['is_climaterisk'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second stage positive paragraphs: 3067\n"
     ]
    }
   ],
   "source": [
    "positive_docs = df[df[constants.cro_category_labels].any(axis=1)].copy()\n",
    "positive_docs[\"processed_docs\"] = positive_docs.apply(lambda x: dataframe_preparation.spacy_tokenizer(x['text']), axis=1)\n",
    "print(f\"Second stage positive paragraphs: {len(positive_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "def get_lexical_diversity(text): \n",
    "    lex = LexicalRichness(text, use_TextBlob=False)\n",
    "    return pd.Series([lex.words, lex.terms, lex.ttr, lex.mtld()])\n",
    "    \n",
    "positive_docs[[\"nwords\", \"nterms\", \"ttr\", \"mtld\"]] = positive_docs.apply(lambda x: get_lexical_diversity(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total labels : 3110\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Total labels : {positive_docs[constants.cro_sub_category_labels].sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Acute                   490\n",
       "Chronic                  41\n",
       "Policy                  254\n",
       "Market \\& Technology    144\n",
       "Reputation              102\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Labels by sub category: \")\n",
    "positive_docs[constants.cro_sub_category_labels].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Frequency per report</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Avg. Lexical diversity per paragraph</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Paragraphs</th>\n",
       "      <th>Reports</th>\n",
       "      <th>Mean</th>\n",
       "      <th>St. Dev.</th>\n",
       "      <th>Max</th>\n",
       "      <th>Words</th>\n",
       "      <th>Terms</th>\n",
       "      <th>TTR</th>\n",
       "      <th>MTLD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Physical risk</th>\n",
       "      <th>Acute</th>\n",
       "      <td>1457</td>\n",
       "      <td>490</td>\n",
       "      <td>2.97</td>\n",
       "      <td>2.81</td>\n",
       "      <td>19</td>\n",
       "      <td>104.77</td>\n",
       "      <td>69.90</td>\n",
       "      <td>0.73</td>\n",
       "      <td>82.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chronic</th>\n",
       "      <td>52</td>\n",
       "      <td>41</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.59</td>\n",
       "      <td>3</td>\n",
       "      <td>55.71</td>\n",
       "      <td>42.56</td>\n",
       "      <td>0.82</td>\n",
       "      <td>77.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Transition risk</th>\n",
       "      <th>Policy</th>\n",
       "      <td>977</td>\n",
       "      <td>254</td>\n",
       "      <td>3.85</td>\n",
       "      <td>6.02</td>\n",
       "      <td>39</td>\n",
       "      <td>98.76</td>\n",
       "      <td>65.36</td>\n",
       "      <td>0.72</td>\n",
       "      <td>75.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Market \\&amp; Technology</th>\n",
       "      <td>419</td>\n",
       "      <td>144</td>\n",
       "      <td>2.91</td>\n",
       "      <td>2.65</td>\n",
       "      <td>14</td>\n",
       "      <td>116.94</td>\n",
       "      <td>76.86</td>\n",
       "      <td>0.71</td>\n",
       "      <td>83.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reputation</th>\n",
       "      <td>205</td>\n",
       "      <td>102</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.55</td>\n",
       "      <td>9</td>\n",
       "      <td>76.90</td>\n",
       "      <td>53.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>82.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        Frequency per report  \\\n",
       "                                     Paragraphs Reports                 Mean   \n",
       "                                                                               \n",
       "Physical risk   Acute                      1457     490                 2.97   \n",
       "                Chronic                      52      41                 1.27   \n",
       "Transition risk Policy                      977     254                 3.85   \n",
       "                Market \\& Technology        419     144                 2.91   \n",
       "                Reputation                  205     102                 2.01   \n",
       "\n",
       "                                                   \\\n",
       "                                     St. Dev. Max   \n",
       "                                                    \n",
       "Physical risk   Acute                    2.81  19   \n",
       "                Chronic                  0.59   3   \n",
       "Transition risk Policy                   6.02  39   \n",
       "                Market \\& Technology     2.65  14   \n",
       "                Reputation               1.55   9   \n",
       "\n",
       "                                     Avg. Lexical diversity per paragraph  \\\n",
       "                                                                    Words   \n",
       "                                                                            \n",
       "Physical risk   Acute                                              104.77   \n",
       "                Chronic                                             55.71   \n",
       "Transition risk Policy                                              98.76   \n",
       "                Market \\& Technology                               116.94   \n",
       "                Reputation                                          76.90   \n",
       "\n",
       "                                                          \n",
       "                                      Terms   TTR   MTLD  \n",
       "                                                          \n",
       "Physical risk   Acute                 69.90  0.73  82.37  \n",
       "                Chronic               42.56  0.82  77.03  \n",
       "Transition risk Policy                65.36  0.72  75.69  \n",
       "                Market \\& Technology  76.86  0.71  83.74  \n",
       "                Reputation            53.77  0.77  82.92  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_descriptive = pd.DataFrame()\n",
    "df_tmp = df.groupby(\"report_id\")[constants.cro_sub_category_labels].sum()\n",
    "df_descriptive[\"\", \"Coverage\"] = df_tmp.replace(0, np.nan).count()\n",
    "\n",
    "df_descriptive[\"Frequency per report\", \"Mean\"] = df_tmp.replace(0, np.nan).mean()\n",
    "df_descriptive[\"Frequency per report\", \"St. Dev.\"] = df_tmp.replace(0, np.nan).std()\n",
    "df_descriptive[\"Frequency per report\", \"Max\"] = df_tmp.max()\n",
    "\n",
    "\n",
    "df_descriptive[\"Avg. Lexical diversity per paragraph\", \"Words\"] = df_tmp.apply(lambda x: positive_docs[positive_docs[x.name] > 0].nwords.mean()) \n",
    "df_descriptive[\"Avg. Lexical diversity per paragraph\", \"Terms\"] = df_tmp.apply(lambda x: positive_docs[positive_docs[x.name] > 0].nterms.mean()) \n",
    "df_descriptive[\"Avg. Lexical diversity per paragraph\", \"TTR\"] = df_tmp.apply(lambda x: positive_docs[positive_docs[x.name] > 0].ttr.mean()) \n",
    "df_descriptive[\"Avg. Lexical diversity per paragraph\", \"MTLD\"] = df_tmp.apply(lambda x: positive_docs[positive_docs[x.name] > 0].mtld.mean())\n",
    "\n",
    "# Add tempory\n",
    "df_descriptive[\"Main\"] = [constants.map_to_field()[c[\"parent\"]] for c in constants.cro_sub_categories]\n",
    "\n",
    "df_descriptive.set_index('Main', append=True, inplace=True)\n",
    "df_descriptive.index.set_names(['second', 'first'], inplace=True)\n",
    "df_descriptive = df_descriptive.reorder_levels(['first', 'second'])\n",
    "df_descriptive.index.set_names(['', ''], inplace=True)\n",
    "df_descriptive = df_descriptive.round(2)\n",
    "latex_str = tables.export_to_latex(df_descriptive, \n",
    "                       filename=\"pos_paragraphs_descriptive_stats_v2.tex\",\n",
    "                       correct_multicolumn=True,\n",
    "                       make_bold_row_at=[3],\n",
    "                       # add_verticalrule_at=[3, 6],\n",
    "                       index=True, \n",
    "                       multirow=False, \n",
    "                       multicolumn=False, \n",
    "                       bold_rows=True, \n",
    "                       multicolumn_format=\"l\", \n",
    "                       escape=False,\n",
    "                       float_format=\"{:0.2f}\".format\n",
    "                      )\n",
    "df_descriptive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction evaluation (textual and distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logit, softmax\n",
    "positive_docs[\"Positive Probability\"] = positive_docs[\"1stage_preds_prob\"].apply(lambda x: softmax(logit(x)))\n",
    "positive_docs[\"Positive Probability\"] = positive_docs[\"Positive Probability\"].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Positive Probability\"] = df[\"1stage_preds_prob\"].apply(lambda x: softmax(logit(x)))\n",
    "df[\"Positive Probability\"] = df[\"Positive Probability\"].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Positive Probability\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.kdeplot(positive_docs[\"Positive Probability\"], clip=[0,1], cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_uncertain = positive_docs.sort_values(by=['Positive Probability']).iloc[0]\n",
    "most_certain = positive_docs.sort_values(by=['Positive Probability']).iloc[-1]\n",
    "print(f\"Most uncertain: {most_uncertain.report_id}, {most_uncertain.page_no}, Prob: {most_uncertain['Positive Probability']}\\n\", softmax(logit(most_uncertain[\"2stage_preds_prob\"]))[3] )\n",
    "print(most_uncertain.text)\n",
    "print(\"=====================\")\n",
    "print(f\"Most uncertain: {most_certain.report_id}, {most_certain.page_no}, Prob: {most_certain['Positive Probability']}\\n\", softmax(logit(most_certain[\"2stage_preds_prob\"]))[0] )\n",
    "print(most_certain.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_docs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_docs_long = pd.melt(positive_docs, id_vars=['Positive Probability', '2stage_preds_prob'],  value_vars=constants.cro_sub_category_labels, var_name='Category', value_name='Positive')\n",
    "positive_docs_long = positive_docs_long.query(\"Positive > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx(x):\n",
    "    idx = next((index for (index, d) in enumerate(constants.cro_sub_category_labels) if d == x[\"Category\"]), None)\n",
    "    return idx \n",
    "\n",
    "def get_class_prob(x):\n",
    "    # Fix for inverse logits\n",
    "    probs = softmax(logit(x[\"2stage_preds_prob\"]))\n",
    "    idx = get_idx(x)\n",
    "    return probs[idx]\n",
    "    \n",
    "    \n",
    "positive_docs_long[\"Class probability\"] = positive_docs_long.apply(lambda x: get_class_prob(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [c[\"color\"] for c in constants.cro_sub_categories]\n",
    "      \n",
    "fig = plt.figure()#figsize=(9, 5))\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax1 = fig.add_subplot(122)\n",
    "sns.kdeplot(data=positive_docs_long, ax=ax0, x=\"Class probability\", hue=\"Category\", palette=colors, clip=[0,1], common_norm=False, multiple=\"layer\")\n",
    "sns.ecdfplot(positive_docs_long, ax=ax1, x=\"Class probability\", hue=\"Category\", palette=colors, complementary=True)\n",
    "\n",
    "ax1.get_legend().remove()\n",
    "plt.tight_layout()\n",
    "fig.savefig(os.path.join(export_dir, \"class_probability_distributions.pdf\"), format='pdf', bbox_inches='tight')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution over the years\n",
    "\n",
    "Shows the level of *average number of predicted CR's per report* (ACRR) over time.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def convert_to_long(df, labels):\n",
    "    df_long = df.groupby(['Year', 'report_id']).sum()[labels]\n",
    "    df_long = df_long.reset_index()\n",
    "    df_long = pd.melt(df_long, id_vars=[\"Year\"], value_vars=labels, var_name='Category', value_name='Frequency (per report)')\n",
    "    return df_long\n",
    "\n",
    "def plot_evolution(df, categories, **kwargs):\n",
    "    colors = [c[\"color\"] for c in categories]\n",
    "    df = convert_to_long(df, [c[\"label\"] for c in categories])\n",
    "    ax = sns.lineplot(data=df, x=\"Year\", y=\"Frequency (per report)\", hue=\"Category\", palette=colors, style=\"Category\", **kwargs)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlim()\n",
    "    plt.xlim(min(df.Year), max(df.Year))\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_evolution_categories(df, categories, **kwargs):\n",
    "    ax = plot_evolution(df, categories, **kwargs)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    fig = ax.get_figure()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_grid(groups, column, categories, exclude_groups=[], ncols=4, **kwargs):\n",
    "    nrows = math.ceil(len(groups) / ncols)    \n",
    "    fig = plt.figure(figsize=(12, 15 if nrows > 1 else 5))\n",
    "\n",
    "    shared_y_ax = None\n",
    "    shared_x_ax = None\n",
    "    for idx, c in enumerate(groups):\n",
    "        if c in exclude_groups:\n",
    "            is_excluded = True \n",
    "            sharey = None\n",
    "        else:\n",
    "            is_excluded = False\n",
    "            sharey = shared_y_ax\n",
    "        ax = fig.add_subplot(nrows, ncols, idx + 1, sharey=sharey, sharex=shared_x_ax)\n",
    "        if not shared_y_ax:\n",
    "            shared_x_ax = ax\n",
    "        if not is_excluded:\n",
    "            shared_y_ax = ax\n",
    "        \n",
    "        filtered_df = df.query(f\"{column} == @c\")\n",
    "        ax2 = plot_evolution(filtered_df, categories, ax=ax, **kwargs)\n",
    "        ax2.title.set_text(c)\n",
    "        \n",
    "        if ax.is_last_row():\n",
    "            pass\n",
    "        else:\n",
    "            plt.setp(ax.get_xticklabels(), visible=False)\n",
    "            ax.set_xlabel(None)\n",
    "\n",
    "        if ax.is_first_col():\n",
    "            pass\n",
    "        else:\n",
    "            plt.setp(ax.get_yticklabels(), visible=False)\n",
    "            ax.set_ylabel(None)\n",
    "        \n",
    "        if is_excluded:\n",
    "            plt.setp(ax.get_yticklabels(), visible=True)\n",
    "            ax.yaxis.tick_right()\n",
    "            \n",
    "    return fig\n",
    "    \n",
    "\n",
    "fig = plot_evolution_categories(df, constants.cro_categories)\n",
    "fig.savefig(os.path.join(export_dir, \"levels_acror_cro_years.pdf\"), format='pdf', bbox_inches='tight')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_evolution_categories(df, constants.cro_sub_categories, ci=None)\n",
    "fig.savefig(os.path.join(export_dir, f\"levels_acror_cro_sub_type_years.pdf\"), format='pdf', bbox_inches='tight')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = sorted(df_master.country.unique())\n",
    "all_countries_fig = plot_grid(all_countries, 'country', constants.cro_categories, ncols=3, ci=None)\n",
    "all_countries_fig.savefig(os.path.join(export_dir, f\"levels_acror_cro_countries.pdf\"), format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_countries_fig = plot_grid([\"DE\", \"CH\", \"FR\", \"GB\"], 'country', constants.cro_categories, ncols=4, ci=None)\n",
    "selected_countries_fig.savefig(os.path.join(export_dir, f\"levels_acror_cro_selected_countries.pdf\"), format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_industries = sorted(df_master.icb_industry.unique())\n",
    "all_inudustries_fig = plot_grid(all_industries, 'icb_industry', constants.cro_categories, exclude_groups=[\"Energy\"], ncols=4, ci=None)\n",
    "all_inudustries_fig.savefig(os.path.join(export_dir, f\"levels_acror_cro_industry.pdf\"), format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_industries_fig = plot_grid([\"Consumer Discretionary\", \"Financials\", \"Telecommunications\", \"Energy\"], 'icb_industry', constants.cro_categories, exclude_groups=[\"Energy\"], ncols=4, ci=None)\n",
    "selected_industries_fig.savefig(os.path.join(export_dir, f\"levels_acror_cro_selected_industries.pdf\"), format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_year = 2015\n",
    "labels = [c['label'] for c in constants.cro_categories]\n",
    "colors = [c[\"color\"] for c in constants.cro_categories]\n",
    "\n",
    "df_long = df.query(\"Year >= @from_year\").groupby(['country', 'report_id']).sum()[labels]\n",
    "df_long = df_long.reset_index()\n",
    "print(f\"Number of reports (from {from_year}): {len(df_long.report_id.unique())}\")\n",
    "df_long = pd.melt(df_long, id_vars=[\"country\"], value_vars=labels, var_name='Category', value_name='Frequency (per report)')\n",
    "df_long = df_long.rename(columns={\"country\": \"Country\"})\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "ax = sns.boxplot(y=\"Country\", x=\"Frequency (per report)\", hue=\"Category\", data=df_long, palette=colors)\n",
    "fig = ax.get_figure()\n",
    "# plt.tight_layout()\n",
    "fig.subplots_adjust(left=0.2)\n",
    "fig.savefig(os.path.join(export_dir, f\"cro_country_distribution.pdf\"), format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df.query(\"Year >= @from_year\").groupby(['icb_industry', 'report_id']).sum()[labels]\n",
    "df_long = df_long.reset_index()\n",
    "print(f\"Number of reports (from {from_year}): {len(df_long.report_id.unique())}\")\n",
    "df_long = pd.melt(df_long, id_vars=[\"icb_industry\"], value_vars=labels, var_name='Category', value_name='Frequency (per report)')\n",
    "df_long = df_long.rename(columns={\"icb_industry\": \"Industry\"})\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "ax = sns.boxplot(y=\"Industry\", x=\"Frequency (per report)\", hue=\"Category\", data=df_long, palette=colors)\n",
    "fig = ax.get_figure()\n",
    "#plt.tight_layout()\n",
    "fig.subplots_adjust(left=0.2)\n",
    "fig.savefig(os.path.join(export_dir, f\"cro_industry_distribution.pdf\"), format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_doc = positive_docs[positive_docs[constants.cro_category_labels[0]]]\n",
    "tr_doc = positive_docs[positive_docs[constants.cro_category_labels[1]]]\n",
    "acute_doc = positive_docs[positive_docs[constants.cro_sub_category_labels[0]] > 0]\n",
    "chron_doc = positive_docs[positive_docs[constants.cro_sub_category_labels[1]] > 0]\n",
    "policy_doc = positive_docs[positive_docs[constants.cro_sub_category_labels[2]] > 0]\n",
    "market_doc = positive_docs[positive_docs[constants.cro_sub_category_labels[3]] > 0]\n",
    "reputation_doc = positive_docs[positive_docs[constants.cro_sub_category_labels[4]] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most frequent ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams, FreqDist\n",
    "\n",
    "def get_word_counts(docs, most_common=50):\n",
    "    counts = dict()\n",
    "    for size in 1, 2:\n",
    "        counts[size] = FreqDist(ngrams(docs, size))\n",
    "        \n",
    "    counts_df = pd.DataFrame(data=counts[1].most_common(most_common), columns=[\"Unigram\", \"Frequency\"]) # columns=pd.MultiIndex.from_tuples([('Unigram', ''), ('Unigram', 'Frequency')]))\n",
    "    # counts_df = counts_df.sort_values(by=['Frequency'], ascending=False)\n",
    "    counts_df = counts_df.join(\n",
    "        pd.DataFrame(data=counts[2].most_common(most_common), columns=['Bigram', \"Frequency2\"]) # columns=pd.MultiIndex.from_tuples([('Bigram', ''), ('Bigram', 'Frequency')]))\n",
    "    )\n",
    "    counts_df = counts_df.head(most_common)\n",
    "    \n",
    "    counts_df['Unigram'] = counts_df['Unigram'].apply(lambda x: \" \".join(x))\n",
    "    counts_df['Bigram'] = counts_df['Bigram'].apply(lambda x: \" \".join(x))\n",
    "    counts_df = counts_df.rename(columns={\"Frequency2\": \"Frequency\"})\n",
    "    return counts_df\n",
    "    \n",
    "\n",
    "most_frequent_df = get_word_counts(positive_docs['processed_docs'].explode(), most_common=20)\n",
    "tables.export_to_latex(most_frequent_df, filename=\"most_frequent_terms.tex\", make_bold_row_at=2, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(positive_docs[\"processed_docs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.9, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gen_wordcloud(docs, export_path, **params):\n",
    "    input_text = \" \".join(docs)\n",
    "    wordcloud = WordCloud(background_color=\"white\", relative_scaling=0.6, **params).generate(input_text)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    if export_path:\n",
    "        wordcloud.to_file(export_path + \".pdf\")\n",
    "        svg = wordcloud.to_svg()\n",
    "        # with open(export_path + \".svg\", \"wt\") as f:\n",
    "        #     f.write(svg)\n",
    "\n",
    "wordclouds_path = os.path.join(export_dir, \"wordclouds\")\n",
    "gen_wordcloud(pr_doc[\"text\"], os.path.join(wordclouds_path, \"pr\"), scale=1, height=400, width=800)\n",
    "gen_wordcloud(tr_doc[\"text\"], os.path.join(wordclouds_path, \"tr\"), scale=1, height=400, width=800)\n",
    "\n",
    "for c in constants.cro_sub_categories:\n",
    "    docs = positive_docs[positive_docs[c[\"label\"]] > 0]\n",
    "    path = os.path.join(wordclouds_path, c[\"code\"].lower())\n",
    "    wordcloud = gen_wordcloud(docs[\"text\"], path, scale=1, height=400, width=600)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
